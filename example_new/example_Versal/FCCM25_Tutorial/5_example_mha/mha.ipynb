{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§® Demonstration: Integrating PL and AIE Modules\n",
    "\n",
    "- This example showcases how **ARIES** enables integration between a **PL (Programmable Logic)** kernel and an **AIE (AI Engine)** kernel. The PL kernel is implemented as a reusable MLIR-based library. \n",
    "\n",
    "- The integration highlights ARIES's ability to provide a **unified representation** for both PL and AIE components.\n",
    "\n",
    "- Additionally, this example illustrates **component-aware optimizations**. ARIES will continue to evolve toward **holistic optimization** strategies that consider both PL and AIE together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "cur_dir = os.getcwd()\n",
    "aries_path = cur_dir + \"/../../../../\"\n",
    "sys.path.append(aries_path)\n",
    "from frontend import *\n",
    "from IPython import get_ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention: BMM â†’ Softmax â†’ BMM\n",
    "\n",
    "The dataflow graph below illustrates the structure of a multi-head attention operation, consisting of a batched matrix multiplication followed by a softmax and another batched matrix multiplication. In this pipeline, the output of each stage serves as the input to the next.\n",
    "\n",
    "<img src=\"../images/bmm_softmax_bmm.png\" alt=\"Multi-Head Attention\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MHA0:  Q_K[heads] = Q[heads][seq][head_dim] * K[heads][head_dim][seq]\n",
    "# Softmax:  temp[heads][seq][seq] = softmax(Q_K, dim=3) / (head_dim) ^ (1/2)\n",
    "# MHA1: out[heads][seq][head_dim] = temp[heads][seq][seq] * V[heads][seq][head_dim]\n",
    "\n",
    "SEQ = 64\n",
    "HEADS = 4\n",
    "HEAD_DIM = 64\n",
    "HIDDEN = HEADS * HEAD_DIM\n",
    "\n",
    "TB, TI, TJ, TK = 1, 32, 32, 32\n",
    "GRID_B0, GRID_I0, GRID_J0, GRID_K0 = HEADS//TB, SEQ // TI, HEAD_DIM // TJ, SEQ // TK\n",
    "GRID_B1, GRID_I1, GRID_J1, GRID_K1 = HEADS//TB, SEQ // TI, SEQ // TJ, HEAD_DIM // TK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ”„ Execution Flow: BMM â†’ Softmax â†’ BMM\n",
    "\n",
    "In this setup, the **batch matrix multiplication (BMM)** layers are executed on the **AIE overlay**, while the **Softmax** operation is implemented as an **MLIR library** and executed on the **PL (Programmable Logic)**.\n",
    "\n",
    "Intermediate data between the BMM and Softmax layers is communicated via **L3 external memory**.\n",
    "\n",
    "<img src=\"../images/pl_aie_model.png\" alt=\"pl_aie\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_kernel(external_path=\"aie1/adf/kernel_mm/aie_fp32_v0\", para = [TI, TJ, TK])\n",
    "def kernel_gemm(TileA: float32[TB, TI, TK], \n",
    "                TileB: float32[TB, TK, TJ], \n",
    "                TileC: float32[TB, TI, TJ]):\n",
    "    for tb in range(0, TB):\n",
    "      for i0 in range(0, TI):\n",
    "          for j0 in range(0, TJ):\n",
    "              TileC[tb, i0, j0] = float32(0)\n",
    "              for k0 in range(0, TK):\n",
    "                  TileC[tb, i0, j0] += TileA[tb, i0, k0] * TileB[tb, k0, j0]\n",
    "\n",
    "@task_tile()\n",
    "def bmm(A: float32[-1, -1, -1], B: float32[-1, -1, -1], \n",
    "        C: float32[-1, -1, -1], GRID_B, GRID_I, GRID_J, GRID_K):\n",
    "    \n",
    "    for b in range(GRID_B):\n",
    "      for i in range(GRID_I):\n",
    "          for j in range(GRID_J):\n",
    "              for k in range(GRID_K):\n",
    "                  # Compute tile slices for multiple dimensions\n",
    "                  tb = aries.arange(b*TB, (b+1)*TB)  # B tile range\n",
    "                  ti = aries.arange(i*TI, (i+1)*TI)  # I tile range\n",
    "                  tj = aries.arange(j*TJ, (j+1)*TJ)  # J tile range\n",
    "                  tk = aries.arange(k*TK, (k+1)*TK)  # K tile range\n",
    "\n",
    "                  L1_A = aries.buffer((TB, TI, TK), \"float32\")\n",
    "                  L1_B = aries.buffer((TB, TK, TJ), \"float32\")\n",
    "                  L1_C = aries.buffer((TB, TI, TJ), \"float32\")\n",
    "\n",
    "                  L1_A = aries.load(A, (tb, ti, tk))\n",
    "                  L1_B = aries.load(B, (tb, tk, tj))\n",
    "                  kernel_gemm(L1_A, L1_B, L1_C)\n",
    "                  aries.accstore(L1_C, C, (tb, ti, tj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@task_top()\n",
    "def top(Q: float32[HEADS, SEQ, HEAD_DIM], K_Trans: float32[HEADS, HEAD_DIM, SEQ], \n",
    "        QK: float32[HEADS, SEQ, SEQ], SoftM: float32[HEADS, SEQ, SEQ], \n",
    "        V: float32[HEADS, SEQ, HEAD_DIM], OUT: float32[HEADS, SEQ, HEAD_DIM]):\n",
    "    # Cast the Arrays to dynamic shape\n",
    "    cast_Q = aries.cast(Q, (-1, -1, -1)) # This is for lowering\n",
    "    cast_K_Trans = aries.cast(K_Trans, (-1, -1, -1))\n",
    "    cast_QK = aries.cast(QK, (-1, -1, -1))\n",
    "    cast_SoftM = aries.cast(SoftM, (-1, -1, -1))\n",
    "    cast_V = aries.cast(V, (-1, -1, -1))\n",
    "    cast_OUT = aries.cast(OUT, (-1, -1, -1))\n",
    "    \n",
    "    bmm_task = bmm(cast_Q, cast_K_Trans, cast_QK, GRID_B0, GRID_I0, GRID_J0, GRID_K0)\n",
    "    softmax(cast_QK, cast_SoftM)\n",
    "    bmm_task = bmm(cast_SoftM, cast_V, cast_OUT, GRID_B1, GRID_I1, GRID_J1, GRID_K1)\n",
    "    return bmm_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the input cells that contains the decorators\n",
    "cell_codes = get_ipython().user_ns[\"In\"][2:5]\n",
    "# Join them into one string, with a newline between each cell\n",
    "all_code = \"\\n\".join(cell_codes)\n",
    "\n",
    "# Initialize the buffers\n",
    "np.random.seed(0)\n",
    "Q = np.random.rand(HEADS, SEQ, HEAD_DIM).astype(np.float32)\n",
    "K_Trans = np.random.rand(HEADS, HEAD_DIM, SEQ).astype(np.float32)\n",
    "QK = np.zeros((HEADS, SEQ, SEQ)).astype(np.float32)\n",
    "SoftM = np.zeros((HEADS, SEQ, SEQ)).astype(np.float32)\n",
    "V = np.random.rand(HEADS, SEQ, HEAD_DIM).astype(np.float32)\n",
    "OUT = np.zeros((HEADS, SEQ, HEAD_DIM)).astype(np.float32)\n",
    "\n",
    "# Execute on CPU\n",
    "gemm_task = top(Q, K_Trans, QK, SoftM, V, OUT)\n",
    "\n",
    "# Golden file generation\n",
    "golden_QK = np.matmul(Q, K_Trans)\n",
    "golden_SoftM = softmax_sw(golden_QK)\n",
    "golden_OUT = np.matmul(golden_SoftM, V)\n",
    "\n",
    "# Compare the program with golden file\n",
    "print(\"QK matches golden reference:\", np.allclose(QK, golden_QK))\n",
    "print(\"Softmax output matches golden reference:\", np.allclose(SoftM, golden_SoftM))\n",
    "print(\"Final output matches golden reference:\", np.allclose(OUT, golden_OUT))\n",
    "\n",
    "# # Generate files for on-board test\n",
    "aries.gen_sim([Q, K_Trans, golden_QK, golden_SoftM, V, golden_OUT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Although the kernel in PL is currently provided as a library, **ARIES** can still analyze and monitor itâ€”since all kernels are represented in MLIR.\n",
    "\n",
    "- For example, in this scenario where **BMM runs on AIE** and **Softmax runs on PL**, the intermediate data is communicated through external memory. When optimizing the off-chip bandwidth of the BMM kernel by assigning a larger AXI port width, **ARIES ensures that the interface of the Softmax kernel aligns with that of the BMM kernel**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify primitives to optimize hardware design\n",
    "sch = Schedule(gemm_task)\n",
    "sch.to(\"VCK190\")\n",
    "sch.axiWidth(gemm_task, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the project dir and template dir\n",
    "prj_dir= cur_dir + '/project_mha'\n",
    "temp_dir= aries_path + '/templates'\n",
    "# Generate Initial MLIR and ARIES Opts\n",
    "sch.build(all_code, prj_dir, temp_dir)\n",
    "sch.compile(aries_path, prj_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aries",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
